# ğŸ›’ Ecommerce Dashboard on GCP

This project demonstrates an end-to-end Big Data pipeline for ecommerce analytics, leveraging Google Cloud Platform (GCP), PySpark for ETL, and Plotly for interactive dashboards.

---

## ğŸ“Œ Overview

- ğŸ“¦ Data Source: [Kaggle Ecommerce Dataset](https://www.kaggle.com/)
- ğŸš€ Platform: Google Cloud Platform (GCP)
- ğŸ”§ ETL Framework: PySpark
- ğŸ—ƒï¸ Storage: Google Cloud Storage (GCS)
- ğŸ“Š Dashboarding: Python (Plotly)
- â˜ï¸ Deployment: GCP Dataproc + GCS + Local Dashboard

---

## ğŸ”„ Workflow Summary

1. **Data Ingestion**
   - Downloaded the raw ecommerce dataset from Kaggle.
   - Uploaded it to Google Cloud Storage (GCS).

2. **ETL using PySpark**
   - Set up a Dataproc cluster on GCP.
   - Performed data cleaning, transformation, and aggregation using PySpark on Dataproc.
   - Output: Cleaned and processed files saved to GCS in Parquet/CSV format.

3. **Data Storage**
   - Final transformed datasets stored in GCS, optimized for query and analytics.

4. **Dashboard Creation**
   - Used **Plotly** in Python to create interactive visualizations.
   - Built a local dashboard showcasing:
     - ğŸ“ˆ Total Sales Over Time
     - ğŸ“Š Orders by Category
     - ğŸ’° Revenue Trends
     - ğŸ›ï¸ Top-Selling Products
   - Data read from GCS using `pandas` with `gcsfs` or directly through BigQuery.
   - Dashboard served locally via **Plotly Dash** or within **Jupyter Notebook**.

5. **Deployment**
   - ETL pipeline deployed using GCP Dataproc.
   - Dashboard can be hosted locally or on GCP using App Engine / Cloud Run for broader access.

---

## ğŸ› ï¸ Tech Stack

| Component       | Tool/Service       |
|----------------|--------------------|
| Data Source     | Kaggle             |
| Compute Engine  | Dataproc (PySpark) |
| Storage         | Google Cloud Storage |
| Language        | Python (PySpark, Pandas) |
| Dashboard       | Plotly / Dash      |
| Deployment      | GCP                |

---

## ğŸ“ Project Structure

